{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Data Aggregation\n",
    "\n",
    "This dataset loads all `.csv` files and transforms them into one big dataframe.\n",
    "Then, the correct datatypes are induced and the names are set to something more appropriate and easy to work with later.\n",
    "These results are saved in case more fields are wanted later.\n",
    "\n",
    "Next, all non-relevant feature fields are discarded and outliers are filtered, where they are first set to `np.nan` for the resampling and interpolation, but are set to 0.0, eventually, to mimic a natural drop-out.\n",
    "In a final step, the data is saved in a feature numpy array, which can be used as the input data for training and validation."
   ],
   "id": "d922f4bafae36236"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "from src.physics.eclipse import calculate_eclipse_details\n",
    "from src.physics.msis_density import get_nrlm_densities\n",
    "from src.model.scaler import ZTransform"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# constants\n",
    "raw_data_dir = Path(\"../data/raw\")\n",
    "preprocessed_data_dir = Path(\"../data/preprocessed\")\n",
    "aggregated_data_dir = preprocessed_data_dir / \"aggregated\"\n",
    "\n",
    "higher_order_interpolation_limit = 4\n",
    "interpolation_limit = 6\n",
    "fill_limit=8\n",
    "\n",
    "# ensure no error occurs later\n",
    "aggregated_data_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "63c0f49952633059",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OMNI2\n",
    "\n",
    "The first dataset to be prepared is the OMNI2 dataset."
   ],
   "id": "e8415beafde37bfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# omni constants\n",
    "raw_omni_dir = raw_data_dir / \"OMNI2\"\n",
    "preprocessed_omni_path = aggregated_data_dir / \"omni.parquet\"\n",
    "\n",
    "# discard unnecessary fields\n",
    "filtered_omni_path = aggregated_data_dir / \"omni-filtered.parquet\"\n"
   ],
   "id": "97f1f95b7acfa8b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# aggregate into one file\n",
    "omni_dtypes = {\n",
    "    'YEAR': np.uint16, 'DOY': np.uint16, 'Hour': np.uint8, 'Bartels_rotation_number': np.uint16,\n",
    "    'ID_for_IMF_spacecraft': np.uint16, 'ID_for_SW_Plasma_spacecraft': np.uint16,\n",
    "    'num_points_IMF_averages': np.int16, 'num_points_Plasma_averages': np.int16,\n",
    "    'Scalar_B_nT': np.float32, 'Vector_B_Magnitude_nT':np.float32,\n",
    "    'Lat_Angle_of_B_GSE': np.float32, 'Long_Angle_of_B_GSE': np.float32,\n",
    "    'BX_nT_GSE_GSM': np.float32, 'BY_nT_GSE': np.float32, 'BZ_nT_GSE': np.float32, 'BY_nT_GSM': np.float32, 'BZ_nT_GSM': np.float32,\n",
    "    'RMS_magnitude_nT': np.float32, 'RMS_field_vector_nT': np.float32, 'RMS_BX_GSE_nT': np.float32, 'RMS_BY_GSE_nT': np.float32, 'RMS_BZ_GSE_nT': np.float32,\n",
    "    'SW_Plasma_Temperature_K': np.float64, 'SW_Proton_Density_N_cm3': np.float32, 'SW_Plasma_Speed_km_s': np.float32, 'SW_Plasma_flow_long_angle': np.float32, 'SW_Plasma_flow_lat_angle': np.float32,\n",
    "    'Alpha_Prot_ratio': np.float32,\n",
    "    'sigma_T_K': np.float64, 'sigma_n_N_cm3': np.float32, 'sigma_V_km_s': np.float32, 'sigma_phi_V_degrees': np.float32, 'sigma_theta_V_degrees': np.float32, 'sigma_ratio': np.float32,\n",
    "    'Flow_pressure': np.float32, 'E_electric_field': np.float32, 'Plasma_Beta': np.float32, 'Alfen_mach_number': np.float32, 'Magnetosonic_Mach_number': np.float32, 'Quasy_Invariant': np.float32,\n",
    "    'Kp_index': np.float32, 'R_Sunspot_No': np.float32,\n",
    "    'Dst_index_nT': np.float32, 'ap_index_nT': np.float32, 'f10.7_index': np.float32, 'AE_index_nT': np.float32, 'AL_index_nT': np.float32, 'AU_index_nT': np.float32, 'pc_index': np.float32, 'Lyman_alpha': np.float32,\n",
    "    'Proton_flux_>1_Mev': np.float32, 'Proton_flux_>2_Mev': np.float32, 'Proton_flux_>4_Mev': np.float32,\n",
    "    'Proton_flux_>10_Mev': np.float32, 'Proton_flux_>30_Mev': np.float32, 'Proton_flux_>60_Mev': np.float32,\n",
    "    'Flux_FLAG': np.int8\n",
    "}\n",
    "\n",
    "read_dfs = {}\n",
    "for file in tqdm(raw_omni_dir.iterdir(), total=len(tuple(raw_omni_dir.iterdir())), desc=\"Reading raw OMNI2 files\"):\n",
    "    if file.is_file():\n",
    "        file_id = int(file.name[6:11])\n",
    "        df = pd.read_csv(file, parse_dates=[\"Timestamp\"], sep=',', index_col=\"Timestamp\", engine='pyarrow').astype(omni_dtypes)\n",
    "        read_dfs[file_id] = df\n",
    "\n",
    "omni_df = pd.concat(read_dfs)\n",
    "omni_df.index.rename(['File ID', 'Timestamp'], inplace=True)\n",
    "omni_df.to_parquet(preprocessed_omni_path)"
   ],
   "id": "21ab2356f056350d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "omni_df = pd.read_parquet(preprocessed_omni_path)\n",
    "omni_df.info()"
   ],
   "id": "ea0e2ec7b02da0b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# look at docs/used_fields_revised.md for more details\n",
    "fields_to_keep = ['Hour', 'Bartels_rotation_number', 'DOY', 'YEAR', 'f10.7_index', 'Kp_index', 'Dst_index_nT', 'Lyman_alpha', 'BY_nT_GSM', 'BZ_nT_GSM', 'Proton_flux_>1_Mev', 'Proton_flux_>2_Mev', 'Proton_flux_>4_Mev', 'Proton_flux_>10_Mev', 'Proton_flux_>30_Mev', 'Proton_flux_>60_Mev', 'SW_Proton_Density_N_cm3', 'SW_Plasma_Speed_km_s', 'AL_index_nT']\n",
    "\n",
    "omni_df = omni_df[fields_to_keep]"
   ],
   "id": "4265fe88081fd9c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "omni_df.describe()",
   "id": "8f566e9eada0fbab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# filter outliers and save as preprocessing target\n",
    "fields_to_filter = ['f10.7_index', 'BY_nT_GSM', 'BZ_nT_GSM', 'Proton_flux_>1_Mev', 'Proton_flux_>2_Mev', 'Proton_flux_>4_Mev', 'Proton_flux_>10_Mev', 'Proton_flux_>30_Mev', 'Proton_flux_>60_Mev', 'SW_Proton_Density_N_cm3', 'SW_Plasma_Speed_km_s', 'AL_index_nT']\n",
    "\n",
    "for field in fields_to_filter:\n",
    "    max_val = omni_df[field].max()\n",
    "    omni_df.loc[omni_df[field] >= max_val - 1e-2 * max_val, field] = np.nan\n",
    "\n",
    "omni_df.to_parquet(filtered_omni_path)"
   ],
   "id": "8fe00c954ab18a56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## GOES\n",
    "\n",
    "Next is GOES.\n",
    "Due to the amount of data being preset, it is directly resampled from 1 minute to 15 minute intervals.\n",
    "This also somewhat helps to somewhat alleviate the massive gaps in the data."
   ],
   "id": "103e5097434d1bd4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# goes constants\n",
    "raw_goes_dir = raw_data_dir / \"GOES\"\n",
    "preprocessed_goes_path = aggregated_data_dir / \"goes.parquet\""
   ],
   "id": "12e6a469e81b0a8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# aggregate into one file after choosing relevant fields and downsampling to 15 min\n",
    "goes_dtypes = {\n",
    "    'xrsa_flux': np.float32,\n",
    "    'xrsb_flux': np.float32,\n",
    "    'xrsa_flag': np.float32,    # some flags are null unfortunately\n",
    "    'xrsb_flag': np.float32\n",
    "}\n",
    "\n",
    "fields_to_keep = ['xrsa_flux', 'xrsb_flux', 'xrsa_flag', 'xrsb_flag']\n",
    "\n",
    "read_dfs = {}\n",
    "goes_files = [fp for intermediate_fp in raw_goes_dir.iterdir() for fp in intermediate_fp.iterdir()]\n",
    "\n",
    "for file in tqdm(goes_files, total=len(goes_files), desc=\"Reading raw GOES files\"):\n",
    "    if file.is_file():\n",
    "        file_id = int(file.name[5:10])\n",
    "        df = pd.read_csv(file, parse_dates=[\"Timestamp\"], sep=',', index_col=\"Timestamp\", engine='pyarrow').astype(goes_dtypes)\n",
    "        df = df[fields_to_keep]\n",
    "\n",
    "        for field in ['xrsa_', 'xrsb_']:\n",
    "            field_name = field + 'flux'\n",
    "            field_flag = field + 'flag'\n",
    "\n",
    "            df.loc[df[field_flag] != 0, field_name] = np.nan\n",
    "\n",
    "        df = df.resample('15min')\n",
    "        df = (df.mean() + df.max()) / 2   # linear combination to capture better extreme events\n",
    "        read_dfs[file_id] = df\n",
    "\n",
    "goes_df = pd.concat(read_dfs)\n",
    "goes_df.index.rename(['File ID', 'Timestamp'], inplace=True)\n",
    "goes_df.to_parquet(preprocessed_goes_path)"
   ],
   "id": "8db1e9e42894d3b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "goes_df = pd.read_parquet(preprocessed_goes_path)\n",
    "goes_df.describe()"
   ],
   "id": "a0b2d5cda6c6b317",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# only keep flux info\n",
    "goes_df = goes_df[['xrsa_flux', 'xrsb_flux']]\n",
    "goes_df.info()"
   ],
   "id": "29a3ab1547869148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Satellite Initial States\n",
    "\n",
    "Next in the line are the satellite initial states."
   ],
   "id": "56f45743f4dfa0f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# initial states constants\n",
    "raw_is_dir = raw_data_dir / \"INITIAL_STATES\"\n",
    "preprocessed_is_path = aggregated_data_dir / \"initial-states.parquet\""
   ],
   "id": "450596423952f551",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load csvs and combine into one dataset\n",
    "is_dtypes = {\n",
    "    'File ID': np.int16,\n",
    "    'Semi-major Axis (km)': np.float64,\n",
    "    'Eccentricity': np.float32,\n",
    "    'Inclination (deg)': np.float32,\n",
    "    'RAAN (deg)': np.float32,\n",
    "    'Argument of Perigee (deg)': np.float32,\n",
    "    'True Anomaly (deg)': np.float32,\n",
    "    'Latitude (deg)': np.float32,\n",
    "    'Longitude (deg)': np.float32,\n",
    "    'Altitude (km)': np.float64\n",
    "}\n",
    "\n",
    "fields_to_keep = ['Altitude (km)', 'Inclination (deg)', 'RAAN (deg)', 'Argument of Perigee (deg)', 'True Anomaly (deg)', 'Eccentricity', 'Semi-major Axis (km)']\n",
    "\n",
    "read_dfs = []\n",
    "is_files = [fp for fp in raw_is_dir.iterdir()]\n",
    "\n",
    "for file in tqdm(is_files, total=len(is_files), desc=\"Reading raw Initial States files\"):\n",
    "    if file.is_file():\n",
    "        df = pd.read_csv(file, parse_dates=[\"Timestamp\"], sep=',', index_col=[\"File ID\", \"Timestamp\"], dtype=goes_dtypes)\n",
    "        df = df[fields_to_keep]\n",
    "        read_dfs.append(df)\n",
    "\n",
    "is_df = pd.concat(read_dfs)\n",
    "is_df.to_parquet(preprocessed_is_path)"
   ],
   "id": "a84595c360c3b234",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "is_df = pd.read_parquet(preprocessed_is_path)\n",
    "is_df.info()"
   ],
   "id": "2df2ad7aeb42572b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sat Density\n",
    "\n",
    "Now for the final pre-downloaded datasets, the ground truth."
   ],
   "id": "893bfe7aaa57f71d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sat density constants\n",
    "raw_sd_dir = raw_data_dir / \"SAT_DENSITY\"\n",
    "preprocessed_sd_path = aggregated_data_dir / \"sat-density.parquet\"\n",
    "interpolated_sd_path = aggregated_data_dir / \"sat-density-interp2.parquet\""
   ],
   "id": "1ba9d06d5f709f2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load csvs and combine into one dataset\n",
    "sd_dtypes = {\n",
    "    'Orbit Mean Density (kg/m^3)': np.float32\n",
    "}\n",
    "\n",
    "fields_to_keep = ['Orbit Mean Density (kg/m^3)']\n",
    "\n",
    "read_dfs = {}\n",
    "sd_files = [fp for fp in raw_sd_dir.iterdir()]\n",
    "\n",
    "for file in tqdm(sd_files, total=len(sd_files), desc=\"Reading raw Sat Density files\"):\n",
    "    if file.is_file():\n",
    "        idx = file.name.index('-', 3) + 1\n",
    "        file_id = int(file.name[idx:idx+5])\n",
    "        df = pd.read_csv(file, parse_dates=[\"Timestamp\"], sep=',', index_col=\"Timestamp\", engine='pyarrow').astype(sd_dtypes)\n",
    "        max_val = df['Orbit Mean Density (kg/m^3)'].max()\n",
    "        df.loc[df['Orbit Mean Density (kg/m^3)'] >= max_val - 1e-2 * max_val, 'Orbit Mean Density (kg/m^3)'] = np.nan\n",
    "        read_dfs[file_id] = df\n",
    "\n",
    "sd_df = pd.concat(read_dfs)\n",
    "sd_df.index.rename(['File ID', 'Timestamp'], inplace=True)\n",
    "sd_df.to_parquet(preprocessed_sd_path)"
   ],
   "id": "bcc89c1f925559dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sd_df = pd.read_parquet(preprocessed_sd_path)\n",
    "sd_df.describe()"
   ],
   "id": "32e446b484cfc76f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def _interpolate_group(\n",
    "        group: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Interpolate and refine a single group.\"\"\"\n",
    "    # if group.isna().sum().item() >= len(group) / (1 + interpolation_limit):\n",
    "    #     return group\n",
    "\n",
    "    group = group.sort_index()\n",
    "    orig_idx = group.index\n",
    "    group.index = orig_idx.droplevel(0)\n",
    "\n",
    "    group = group.interpolate(method=\"pchip\", limit_direction=\"both\", limit=higher_order_interpolation_limit)\n",
    "    group = group.interpolate(method=\"linear\", limit_direction=\"both\", limit=interpolation_limit)\n",
    "\n",
    "    group.bfill(inplace=True, limit=fill_limit)\n",
    "    group.ffill(inplace=True, limit=fill_limit)\n",
    "\n",
    "    group.index = orig_idx\n",
    "\n",
    "    return group\n",
    "\n",
    "freq = pd.Timedelta(minutes=10)\n",
    "def _pad_group(group: pd.DataFrame,\n",
    "               target_length: int) -> pd.DataFrame:\n",
    "    \"\"\"Pad a single group to target_length.\"\"\"\n",
    "    cur_len = len(group)\n",
    "    pad_len = target_length - cur_len\n",
    "\n",
    "    if pad_len == 0:\n",
    "        return group\n",
    "    elif pad_len < 0:\n",
    "        return group.tail(target_length)\n",
    "\n",
    "    group = group.sort_index()\n",
    "\n",
    "    # last_timestamp = group.index.get_level_values(\"Timestamp\").max()\n",
    "\n",
    "    # Calculate the start of the required full window\n",
    "    # The new timestamps should go *before* the existing ones to fill the gap\n",
    "    # new_window_start_time = last_timestamp - (target_length - 1) * freq\n",
    "\n",
    "    # Generate the full set of timestamps for the desired window\n",
    "    # This will be the timestamps for the NaNs + existing data\n",
    "    # full_window_timestamps = pd.date_range(start=new_window_start_time,\n",
    "    #                                         periods=target_length,\n",
    "    #                                         freq=freq)\n",
    "\n",
    "    # Create an empty DataFrame for the full window with NaN values\n",
    "    # padded_df = pd.DataFrame(\n",
    "    #     np.nan,\n",
    "    #     index=full_window_timestamps,\n",
    "    #     columns=group.columns,\n",
    "    #     dtype=group.dtypes.iloc[0] # Use the dtype of the first column\n",
    "    # )\n",
    "\n",
    "    # Overlay the existing data onto this padded DataFrame\n",
    "    # This will correctly align by Timestamp and fill in non-NaN values\n",
    "    # padded_df.update(group.reset_index())\n",
    "    ts = group.index.get_level_values(\"Timestamp\").to_series()\n",
    "    file_id = group.index.get_level_values(\"File ID\")[0]\n",
    "\n",
    "\n",
    "    extra_ts = pd.date_range(\n",
    "        start=ts.iloc[-1] + freq, periods=pad_len, freq=freq\n",
    "    )\n",
    "\n",
    "    extra_idx = pd.MultiIndex.from_product(\n",
    "        [[file_id], extra_ts], names=[\"File ID\", \"Timestamp\"]\n",
    "    )\n",
    "\n",
    "    full_idx = group.index.append(extra_idx)\n",
    "\n",
    "    padded_df = group.reindex(full_idx)\n",
    "    return padded_df\n",
    "\n",
    "def _process_group_batch(\n",
    "        batch_groups: list[tuple[any, pd.DataFrame]],\n",
    "        target_length: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Process a batch of groups.\"\"\"\n",
    "    processed_results = []\n",
    "    for name, group in batch_groups:\n",
    "        padded_group = _pad_group(group, target_length)\n",
    "        interpolated_group = _interpolate_group(\n",
    "            padded_group\n",
    "        )\n",
    "        processed_results.append(interpolated_group)\n",
    "\n",
    "    return pd.concat(processed_results)\n",
    "\n",
    "def pad_and_interpolate_parallel(_df: pd.DataFrame, target_length: int) -> pd.DataFrame:\n",
    "    groups = list(_df.groupby(level='File ID', sort=False))\n",
    "    num_groups = len(groups)\n",
    "    actual_n_jobs = cpu_count()\n",
    "    actual_n_jobs = max(1, actual_n_jobs)\n",
    "    batch_size = max(1, (num_groups // actual_n_jobs) + 1)\n",
    "\n",
    "    group_batches = [groups[i:i + batch_size] for i in range(0, num_groups, batch_size)]\n",
    "\n",
    "    print(\"CPU-count:\", actual_n_jobs)\n",
    "    print(\"Batch size:\", batch_size)\n",
    "    print(\"Number of groups:\", num_groups)\n",
    "    print(\"Number of group batches:\", len(group_batches))\n",
    "    print(\"Groups:\", list((i, i + batch_size) for i in range(0, num_groups, batch_size)) + [(num_groups - (num_groups % batch_size), num_groups)])\n",
    "\n",
    "    processed_batches = Parallel(\n",
    "        n_jobs=-1, backend='loky', prefer='processes', verbose=1, batch_size=1, pre_dispatch=actual_n_jobs\n",
    "    )(\n",
    "        delayed(_process_group_batch)(\n",
    "            batch,\n",
    "            target_length\n",
    "        )\n",
    "        for batch in group_batches #  tqdm(group_batches, desc=f\"Interpolating and Padding for {desc}\")\n",
    "    )\n",
    "\n",
    "    return pd.concat(processed_batches).sort_index()"
   ],
   "id": "d63d1c03b7628b9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sd_df_interp = pad_and_interpolate_parallel(sd_df, 432).clip(lower=1e-14, upper=5e-11)\n",
    "sd_df_interp.to_parquet(interpolated_sd_path)\n",
    "sd_df_interp.describe(), sd_df_interp.isnull().sum().item()"
   ],
   "id": "df5679f817df9b55",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sd_df_interp = pd.read_parquet(interpolated_sd_path)\n",
    "sd_df_interp.describe()"
   ],
   "id": "c46e8270c13820f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Eclipse Simulation\n",
    "\n",
    "We supply our model with an additional Eclipse simulation for umbra and penumbra information."
   ],
   "id": "4b8617c309c81e7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# eclipse constants\n",
    "eclipse_preprocessed_p = aggregated_data_dir / 'eclipse.parquet'"
   ],
   "id": "f05bf1bd84546dfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# put the necessary fields of initial states df into the format I used during the challenge\n",
    "orig_is_df = pd.read_parquet(preprocessed_is_path)\n",
    "\n",
    "orig_is_df.rename(columns={\n",
    "    \"Altitude (km)\": \"altitude\",\n",
    "    \"Semi-major Axis (km)\": \"a\",\n",
    "    \"Eccentricity\": \"e\",\n",
    "    \"Inclination (deg)\": \"i\",\n",
    "    \"RAAN (deg)\": \"RAAN\",\n",
    "    \"Argument of Perigee (deg)\": \"omega\",\n",
    "    \"True Anomaly (deg)\": \"nu\"\n",
    "}, inplace=True)\n",
    "\n",
    "_conversion_factor = np.pi / 180.0\n",
    "orig_is_df.a /= 1_000\n",
    "orig_is_df.i = orig_is_df.i * _conversion_factor\n",
    "orig_is_df.RAAN = orig_is_df.RAAN * _conversion_factor\n",
    "orig_is_df.omega = orig_is_df.omega * _conversion_factor\n",
    "orig_is_df.nu = orig_is_df.nu * _conversion_factor"
   ],
   "id": "3d1e392ed3584923",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prepare eclipse\n",
    "eclipse_details_df = calculate_eclipse_details(orig_is_df, n_steps=10_000, threshold=1e-9, n_workers=None)\n",
    "eclipse_details_df.to_parquet(eclipse_preprocessed_p)"
   ],
   "id": "714f5a1d3d544a62",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load\n",
    "eclipse_details_df = pd.read_parquet(eclipse_preprocessed_p)\n",
    "eclipse_details_df.describe()"
   ],
   "id": "4c758dddea367c4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## NRLMSISE-2.1\n",
    "\n",
    "We also need the baseline."
   ],
   "id": "bc736df89e586f31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# constants\n",
    "nrlm_path = aggregated_data_dir / 'nrlm.parquet'"
   ],
   "id": "402a9f9177096f1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# put the necessary fields of omni df into the format I used during the challenge\n",
    "orig_omni_df = pd.read_parquet(preprocessed_omni_path)\n",
    "\n",
    "orig_omni_df.rename(columns={\n",
    "    \"f10.7_index\": \"f10_7_index\",\n",
    "    \"ap_index_nT\": \"ap_index\",\n",
    "}, inplace=True)\n",
    "\n",
    "orig_omni_df = orig_omni_df[['f10_7_index', 'ap_index']]\n",
    "\n",
    "max_val = orig_omni_df.f10_7_index.max()\n",
    "orig_omni_df.loc[orig_omni_df.f10_7_index >= max_val - 1e-2 * max_val, 'f10_7_index'] = np.nan\n",
    "\n",
    "omni_df_interp = orig_omni_df.copy()\n",
    "omni_df_interp['f10_7_index'] = pad_and_interpolate_parallel(orig_omni_df.f10_7_index, 1440)\n",
    "omni_df_interp.describe(), omni_df_interp.isnull().sum(), orig_omni_df.isnull().sum()\n",
    "\n",
    "orig_omni_df = omni_df_interp\n",
    "\n",
    "nrlm_df = get_nrlm_densities(orig_is_df, orig_omni_df, n_steps=1, n_workers=None, mode='relaxed')\n",
    "nrlm_df.to_parquet(nrlm_path)"
   ],
   "id": "449536c139703304",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nrlm_df = pd.read_parquet(nrlm_path)\n",
    "nrlm_df.describe()"
   ],
   "id": "12c9f6ede5d8b98f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Constructing the feature dataset\n",
    "\n",
    "In a final step all previous steps are combined into one big dataset ready for training or evaluation."
   ],
   "id": "9730b9de66b6611f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Sat Density\n",
    "\n",
    "Prepare the final sat density part for the dataset."
   ],
   "id": "7a8f8d509009d5fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sat density\n",
    "is_df = pd.read_parquet(preprocessed_is_path)\n",
    "is_df = is_df.rename(columns={\n",
    "    'Semi-major Axis (km)': 'a',\n",
    "    'Altitude (km)': 'h',\n",
    "    'Inclination (deg)': 'i',   # optional value (we will probably keep it for the time being\n",
    "    'RAAN (deg)': 'RAAN',\n",
    "    'Argument of Perigee (deg)': 'omega',\n",
    "    'True Anomaly (deg)': 'nu',\n",
    "})\n",
    "\n",
    "# we assume almost perfect circular orbits (e ~= 0) and a round earth for nan values\n",
    "threshold = 1e5\n",
    "is_df.loc[is_df.h > threshold, 'h'] = is_df.loc[is_df.h > threshold, 'a'] - 6378\n",
    "\n",
    "_conversion_factor = np.pi / 180.0\n",
    "is_df['orbit_angle'] = (is_df['omega'] + is_df['nu']) * _conversion_factor\n",
    "is_df.RAAN *= _conversion_factor\n",
    "is_df.i *= _conversion_factor\n",
    "\n",
    "is_npy = is_df[['h', 'i', 'RAAN', 'orbit_angle']].sort_index(level=0).to_numpy()\n",
    "print(is_npy.shape)"
   ],
   "id": "b26fc4fbb370b6ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### OMNI2\n",
    "\n",
    "Prepare the final OMNI2 part for the dataset."
   ],
   "id": "ce920d08dfb48e1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load filtered OMNI\n",
    "omni_df = pd.read_parquet(filtered_omni_path)\n",
    "omni_df.describe()"
   ],
   "id": "387a3c036dc1bca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fields_to_keep = [\n",
    "    'year', 'doy', 'bartels_no', 'f10_7_index', 'kp_index', 'dst_index',\n",
    "    'lyman_alpha', 'by', 'bz', 'proton_density', 'plasma_speed',\n",
    "    # optional fields?\n",
    "    'proton_flux_1', 'proton_flux_2', 'proton_flux_4', 'proton_flux_10', 'proton_flux_10', 'proton_flux_30', 'proton_flux_60', 'al_index', 'hour'\n",
    "]\n",
    "\n",
    "omni_df = omni_df.rename(columns={\n",
    "    'YEAR': 'year',\n",
    "    'DOY': 'doy',\n",
    "    'Hour': 'hour',\n",
    "    'Bartels_rotation_number': 'bartels_no',\n",
    "    'f10.7_index': 'f10_7_index',\n",
    "    'Kp_index': 'kp_index',\n",
    "    'Dst_index_nT': 'dst_index',\n",
    "    'Lyman_alpha': 'lyman_alpha',\n",
    "    'BY_nT_GSM': 'by',\n",
    "    'BZ_nT_GSM': 'bz',\n",
    "    'SW_Proton_Density_N_cm3': 'proton_density',\n",
    "    'SW_Plasma_Speed_km_s': 'plasma_speed',\n",
    "    'Proton_flux_>1_Mev': 'proton_flux_1',\n",
    "    'Proton_flux_>2_Mev': 'proton_flux_2',\n",
    "    'Proton_flux_>4_Mev': 'proton_flux_4',\n",
    "    'Proton_flux_>10_Mev': 'proton_flux_10',\n",
    "    'Proton_flux_>30_Mev': 'proton_flux_30',\n",
    "    'Proton_flux_>60_Mev': 'proton_flux_60',\n",
    "    'AL_index_nT': 'al_index'\n",
    "})[fields_to_keep]\n",
    "\n",
    "norm_factor = 2 * np.pi\n",
    "omni_df.bartels_no = np.sin(omni_df.bartels_no * norm_factor)\n",
    "omni_df.doy = np.sin(omni_df.doy * norm_factor / 365.25)\n",
    "omni_df.hour = np.sin(omni_df.hour * norm_factor / 24)\n",
    "omni_df['solar_cycle'] = np.sin(omni_df.year * norm_factor / 11)"
   ],
   "id": "9c0130c76483eddb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get unique File IDs in the desired order\n",
    "unique_file_ids = omni_df.index.get_level_values(\"File ID\").unique().sort_values()\n",
    "\n",
    "# List to store 1D NumPy arrays for each feature/aggregation type\n",
    "all_feature_arrays = []\n",
    "\n",
    "# 1. Get first element of each Subseries for: [\"bartels_no\", \"doy\", \"solar_cycle\"]\n",
    "first_elements_df = omni_df.groupby(level=\"File ID\")[[\"bartels_no\", \"doy\", \"solar_cycle\"]].last().reindex(unique_file_ids)\n",
    "all_feature_arrays.append(first_elements_df.values) # Convert DataFrame to NumPy array\n",
    "\n",
    "def aggregate_time_window_numpy(dataframe, columns, window_str, file_ids_order):\n",
    "    last_timestamps = dataframe.groupby(level='File ID').apply(lambda x: x.index.get_level_values('Timestamp').max()).reindex(file_ids_order)\n",
    "\n",
    "    # Pre-allocate array for results for this aggregation type\n",
    "    # Each column will have 3 or now 4 aggregations (max, mean, std, (min))\n",
    "    num_output_cols = len(columns) * 3\n",
    "    # num_output_cols = len(columns) * 4\n",
    "    results_array = np.full((len(file_ids_order), num_output_cols), np.nan, dtype=np.float64)\n",
    "\n",
    "    for i, file_id in tqdm(enumerate(file_ids_order), desc=\"Aggregating OMNI\"):\n",
    "        last_ts = last_timestamps.loc[file_id]\n",
    "        window_start = last_ts - pd.to_timedelta(window_str)\n",
    "        file_df = dataframe.loc[file_id]\n",
    "        windowed_data = file_df.loc[window_start:last_ts, columns]\n",
    "\n",
    "        if not windowed_data.empty:\n",
    "            agg_result = windowed_data.agg(['max', 'mean', 'std'])\n",
    "            # agg_result = windowed_data.agg(['max', 'mean', 'std', 'min'])\n",
    "            # agg_result is a DataFrame, convert its values to a 1D array row-wise\n",
    "            # This order (max, mean, std for col1, then max, mean, std for col2, etc.)\n",
    "            # matches the desired flattening\n",
    "            results_array[i, :] = agg_result.values.flatten()\n",
    "        # If windowed_data is empty, the pre-allocated np.nan values remain\n",
    "\n",
    "    return results_array\n",
    "\n",
    "# 2. Get max, mean, std for each Subseries for last 14d, 3d, last 3h for:\n",
    "#    [\"f10_7_index\", \"kp_index\", \"dst_index\", \"lyman_alpha\", \"proton_density\", \"plasma_speed\"]\n",
    "fields_long_window = [\"f10_7_index\", \"kp_index\", \"dst_index\", \"lyman_alpha\", \"proton_density\", \"plasma_speed\"]\n",
    "time_windows_long = [\"14d\", \"3d\", \"3h\"]\n",
    "for window in time_windows_long:\n",
    "    agg_array = aggregate_time_window_numpy(omni_df, fields_long_window, window, unique_file_ids)\n",
    "    all_feature_arrays.append(agg_array)\n",
    "\n",
    "# 3. Get max, mean, std for each Subseries for last 24h, 3h for: [\"by\", \"bz\"]\n",
    "fields_short_window = [\"by\", \"bz\"]\n",
    "time_windows_short = [\"24h\", \"3h\"]\n",
    "for window in time_windows_short:\n",
    "    agg_array = aggregate_time_window_numpy(omni_df, fields_short_window, window, unique_file_ids)\n",
    "    all_feature_arrays.append(agg_array)\n",
    "\n",
    "def resample_f10_7_numpy(dataframe, window_str=\"60d\", file_ids_order=unique_file_ids):\n",
    "    last_timestamps = dataframe.groupby(level='File ID').apply(lambda x: x.index.get_level_values('Timestamp').max()).reindex(file_ids_order)\n",
    "\n",
    "    results_array = np.full((len(file_ids_order), 3), np.nan, dtype=np.float64) # 3 for mean, std, max\n",
    "\n",
    "    for i, file_id in enumerate(file_ids_order):\n",
    "        last_ts = last_timestamps.loc[file_id]\n",
    "        window_start = last_ts - pd.to_timedelta(window_str)\n",
    "        file_df = dataframe.loc[file_id]\n",
    "        windowed_data = file_df.loc[window_start:last_ts, \"f10_7_index\"]\n",
    "\n",
    "        if not windowed_data.empty:\n",
    "            mean_val = windowed_data.mean()\n",
    "            std_val = windowed_data.std()\n",
    "            max_val = windowed_data.max()\n",
    "            results_array[i, :] = [mean_val, std_val, max_val]\n",
    "        # If windowed_data is empty, the pre-allocated np.nan values remain\n",
    "\n",
    "    return results_array\n",
    "\n",
    "# 4. Resample f10_7_index to full 60d window with mean, std, and max aggregations\n",
    "f10_7_resampled_array = resample_f10_7_numpy(omni_df, \"60d\", unique_file_ids)\n",
    "all_feature_arrays.append(f10_7_resampled_array)\n",
    "\n",
    "# Combine all feature arrays horizontally\n",
    "omni_npy = np.hstack(all_feature_arrays)\n",
    "print(omni_npy.shape)"
   ],
   "id": "129d0704afbcd313",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### GOES\n",
    "\n",
    "Prepare features for GOES dataset."
   ],
   "id": "828f356efead5fbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load\n",
    "goes_df = pd.read_parquet(preprocessed_goes_path)\n",
    "goes_df.describe()"
   ],
   "id": "9a3c92120851de09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prepare\n",
    "goes_df = goes_df[['xrsa_flux', 'xrsb_flux']]\n",
    "all_feature_arrays = []\n",
    "\n",
    "for window_str in ['14d', '3d', '3h']:\n",
    "    agg_array = aggregate_time_window_numpy(goes_df, ['xrsa_flux', 'xrsb_flux'], window_str, unique_file_ids)\n",
    "    all_feature_arrays.append(agg_array)\n",
    "\n",
    "goes_npy = np.hstack(all_feature_arrays)"
   ],
   "id": "1a8ed2fe0dba8f1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "goes_npy.shape",
   "id": "d7cb8e6878850ecc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Eclipse\n",
    "\n",
    "Prepare the eclipse simulation."
   ],
   "id": "25d12491b3b335f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load\n",
    "eclipse_df = pd.read_parquet(eclipse_preprocessed_p)\n",
    "eclipse_df.describe()"
   ],
   "id": "4421123daeb884d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prepare\n",
    "eclipse_npy = eclipse_df[['penumbra_entry_nu', 'penumbra_exit_nu', 'umbra_entry_nu', 'umbra_exit_nu']].to_numpy()\n",
    "\n",
    "eclipse_npy += is_df.omega.to_numpy().reshape(-1, 1)\n",
    "print(eclipse_npy.shape)"
   ],
   "id": "fa4c41aee5d49e90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Residual targets\n",
    "\n",
    "For the residual targets we need both the given ground truth and the NRLMSISE-2.1 predictions."
   ],
   "id": "c570f2bc07add075"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load\n",
    "nrlm_df = pd.read_parquet(nrlm_path)\n",
    "sd_df = pd.read_parquet(interpolated_sd_path)\n",
    "\n",
    "# sd_df.sort_index().to_numpy().reshape(len(unique_file_ids), -1)\n",
    "# fixed this already in the interpolation step\n",
    "# sd_df = sd_df.groupby(level='File ID').tail(432)    # some are malformatted... -.-\n",
    "sd_df.describe()"
   ],
   "id": "6a235fb66a12cead",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# combine residuals\n",
    "nrlm_npy = nrlm_df.to_numpy()\n",
    "unique_file_ids = sd_df.index.get_level_values(\"File ID\").unique().sort_values()\n",
    "y_npy = sd_df.to_numpy().reshape(len(unique_file_ids), -1) - nrlm_npy"
   ],
   "id": "174f4cddc94b00d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "y_npy",
   "id": "68bd55eeef8cd3f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bad_ys = np.unique(np.argwhere(np.isnan(y_npy))[:, 0])\n",
    "print(len(bad_ys))"
   ],
   "id": "f251d7d6f872d113",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Combine everything\n",
    "\n",
    "Combining all previous steps into one final dataset."
   ],
   "id": "7d3c57b4e55e6a7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# combine\n",
    "final_npy = np.hstack([\n",
    "    is_npy,\n",
    "    omni_npy,\n",
    "    goes_npy,\n",
    "    eclipse_npy\n",
    "])\n",
    "\n",
    "# exclude bad ys\n",
    "good_final_npy = np.delete(final_npy, bad_ys, axis=0)\n",
    "good_nrlm_npy = np.delete(nrlm_npy, bad_ys, axis=0)\n",
    "good_y_npy = np.delete(y_npy, bad_ys, axis=0)\n",
    "\n",
    "np.nan_to_num(good_final_npy, nan=0.0, posinf=0.0, neginf=0.0, copy=False)\n",
    "\n",
    "print(good_final_npy.shape)"
   ],
   "id": "7ae9ab3c2eb54c37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# apply z transform\n",
    "base_z_scaler_path = Path('../model/z-scaler/')\n",
    "feature_z_scaler_path = base_z_scaler_path / \"feature-scaler-scaled-time-with-min.npz\"\n",
    "x_scaler = ZTransform(good_final_npy)\n",
    "x_scaler.save(feature_z_scaler_path)\n",
    "\n",
    "ground_truth_path = base_z_scaler_path / \"ground-truth-scaler-scaled-time-with-min.npz\"\n",
    "y_scaler = ZTransform(good_y_npy)\n",
    "y_scaler.save(ground_truth_path)"
   ],
   "id": "8a4e9a5e86c5f19c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save final dataset\n",
    "dataset_path = preprocessed_data_dir / \"dataset-scaled-time-with-min.npz\"\n",
    "np.savez(file=dataset_path,\n",
    "         x=x_scaler.z_transform(good_final_npy), y=y_scaler.z_transform(good_y_npy), nrlm=good_nrlm_npy)"
   ],
   "id": "8c22b1c0eb119a71",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
