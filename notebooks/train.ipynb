{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "This notebook contains the training script for the revised encoder-decoder transformer with handpicked input features."
   ],
   "id": "4055d05730c42179"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# imports\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import os\n",
    "# os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "# os.environ[\"TORCH_LOGS\"] = \"+dynamo\"\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# import torch.multiprocessing as mp\n",
    "import torchvision.transforms.v2\n",
    "\n",
    "from torch import nn, GradScaler, autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchinfo import summary\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "from src.model.loss import ODRMSELoss\n",
    "from src.model.residual import Transformer\n",
    "from src.model.scaler import ZTransform\n",
    "\n",
    "# mp.set_start_method('spawn', force=True)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# paths\n",
    "dataset_path = Path('../data/preprocessed/dataset-scaled-time.npz')\n",
    "\n",
    "model_path = Path('../model/')\n",
    "ground_truth_scaler_p = model_path / 'z-scaler' / 'ground-truth-scaler-scaled-time.npz'\n",
    "weight_save_path = model_path / 'models'\n",
    "\n",
    "weight_save_path.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "f584c77a306542a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# scaler\n",
    "ground_truth_scaler = ZTransform.load(ground_truth_scaler_p)  # necessary for calculating the OD-RMSE-Loss"
   ],
   "id": "90759da41eda6311",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dataloader constants\n",
    "DEVICE = 'cuda'\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 512\n",
    "\n",
    "INPUT_NOISE_STD = 0.01\n",
    "OUTPUT_NOISE_STD = 0.0005\n",
    "\n",
    "NUM_WORKERS_TRAIN = 24\n",
    "NUM_WORKERS_VALID = 8\n",
    "PERSISTENT_WORKERS = True\n",
    "PREFETCH_FACTOR = 2\n",
    "\n",
    "RESIDUAL_TRAINING = True\n",
    "INCLUDE_NLRMSIS_X = True\n",
    "USE_LOG = True  # requires rescaling before using with OD-RMSE\n",
    "USE_LOG_X = USE_LOG or False"
   ],
   "id": "8270163937bdf616",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load data\n",
    "seed = 0xC0FFEEBABE\n",
    "\n",
    "npz_file_contents = np.load(dataset_path, allow_pickle=True)\n",
    "x, y, baseline = npz_file_contents['x'], npz_file_contents['y'], npz_file_contents['nrlm']\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "idx = rng.permutation(x.shape[0])\n",
    "x = x[idx]\n",
    "y = y[idx]\n",
    "baseline = baseline[idx]\n",
    "\n",
    "total_sims = x.shape[0]\n",
    "split_seperator = int(total_sims * TRAIN_SPLIT)\n",
    "x_train, x_valid = x[:split_seperator], x[split_seperator:]\n",
    "y_train, y_valid = y[:split_seperator], y[split_seperator:]\n",
    "baseline_train, baseline_valid = baseline[:split_seperator], baseline[split_seperator:]"
   ],
   "id": "51cb2bd6b7c81b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baseline_residual = baseline[:, 0].reshape(-1, 1) if RESIDUAL_TRAINING else 0.0\n",
    "y_rescaled = ground_truth_scaler.reverse_z_transform(y) + baseline\n",
    "\n",
    "if USE_LOG:\n",
    "    # we take the log of y not the residual -> can lead to negative values which are not supported by log(x)\n",
    "    y_rescaled = np.log(y_rescaled) - (np.log(baseline_residual) if RESIDUAL_TRAINING else 0.0)\n",
    "    baseline = np.log(baseline)\n",
    "    baseline_train, baseline_valid = baseline[:split_seperator], baseline[split_seperator:]\n",
    "else:\n",
    "    y_rescaled -= baseline_residual\n",
    "\n",
    "scale_one = ZTransform(y_rescaled, axis=(0, 1))\n",
    "y_new = np.stack([scale_one.z_transform(y_rescaled), baseline], axis=-1)\n",
    "\n",
    "y_train, y_valid = y_new[:split_seperator], y_new[split_seperator:]"
   ],
   "id": "7f70171c0266e25c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "baseline_residual = baseline[:, 0].reshape(-1, 1) if RESIDUAL_TRAINING else 0.0\n",
    "y_rescaled = ground_truth_scaler.reverse_z_transform(y) + baseline\n",
    "\n",
    "if USE_LOG:\n",
    "    # we take the log of y not the residual -> can lead to negative values which are not supported by log(x)\n",
    "    y_rescaled = np.log(y_rescaled) - (np.log(baseline_residual) if RESIDUAL_TRAINING else 0.0)\n",
    "    baseline = np.log(baseline)\n",
    "    baseline_train, baseline_valid = baseline[:split_seperator], baseline[split_seperator:]\n",
    "else:\n",
    "    y_rescaled -= baseline_residual\n",
    "\n",
    "scale_one2 = ZTransform(y_rescaled, axis=(0, 1))\n",
    "y2_new = np.stack([scale_one2.z_transform(y_rescaled), baseline], axis=-1)\n",
    "y2 = y_new\n",
    "y2_train, y2_valid = y2_new[:split_seperator], y2_new[split_seperator:]"
   ],
   "id": "c07fcc093ce6c4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train with baseline[0] feature\n",
    "if INCLUDE_NLRMSIS_X:\n",
    "    _x_add_train = baseline_train[:, 0]\n",
    "    _x_add_valid = baseline_valid[:, 0]\n",
    "\n",
    "    if USE_LOG_X and not USE_LOG:\n",
    "        _x_add_train = np.log(_x_add_train)\n",
    "        _x_add_valid = np.log(_x_add_valid)\n",
    "\n",
    "    _baseline_mean = np.mean(_x_add_train)\n",
    "    _baseline_std = np.std(_x_add_train)\n",
    "\n",
    "    rescaled_baseline_train = (_x_add_train - _baseline_mean) / _baseline_std\n",
    "    rescaled_baseline_valid = (_x_add_valid - _baseline_mean) / _baseline_std\n",
    "\n",
    "    x_train = np.concat([x_train, rescaled_baseline_train.reshape(-1, 1)], axis=1)\n",
    "    x_valid = np.concat([x_valid, rescaled_baseline_valid.reshape(-1, 1)], axis=1)\n",
    "    x = np.concat([x_train, x_valid], axis=0)"
   ],
   "id": "9468262bf2baf1e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train with baseline[0] feature\n",
    "x2 = x\n",
    "if INCLUDE_NLRMSIS_X:\n",
    "    _x_add_train = baseline_train[:, 0]\n",
    "    _x_add_valid = baseline_valid[:, 0]\n",
    "\n",
    "    if USE_LOG_X and not USE_LOG:\n",
    "        _x_add_train = np.log(_x_add_train)\n",
    "        _x_add_valid = np.log(_x_add_valid)\n",
    "\n",
    "    _baseline_mean = np.mean(_x_add_train)\n",
    "    _baseline_std = np.std(_x_add_train)\n",
    "\n",
    "    rescaled_baseline_train = (_x_add_train - _baseline_mean) / _baseline_std\n",
    "    rescaled_baseline_valid = (_x_add_valid - _baseline_mean) / _baseline_std\n",
    "\n",
    "    x2_train = np.concat([x_train, rescaled_baseline_train.reshape(-1, 1)], axis=1)\n",
    "    x2_valid = np.concat([x_valid, rescaled_baseline_valid.reshape(-1, 1)], axis=1)\n",
    "    x2 = np.concat([x2_train, x2_valid], axis=0)"
   ],
   "id": "e61dc037baec30d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# dataloader\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, _x, _y, in_transforms=None, out_transforms=None):\n",
    "        self.x = torch.tensor(_x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(_y, dtype=torch.float32)\n",
    "\n",
    "        self.in_transforms = in_transforms\n",
    "        self.out_transforms = out_transforms\n",
    "\n",
    "    def __getitem__(self, _index):\n",
    "        _x = self.x[_index]\n",
    "        if self.in_transforms is not None:\n",
    "            _x = self.in_transforms(_x)\n",
    "\n",
    "        _y = self.y[_index]\n",
    "        if self.out_transforms is not None:\n",
    "            _y = self.out_transforms(_y)\n",
    "\n",
    "        return _x, _y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "    x_train, y_train,  # + ground_truth_scaler.z_transform(baseline[:split_seperator]),\n",
    "    in_transforms=torchvision.transforms.v2.GaussianNoise(sigma=INPUT_NOISE_STD, clip=False),\n",
    "    out_transforms=torchvision.transforms.v2.GaussianNoise(sigma=OUTPUT_NOISE_STD, clip=False)\n",
    ")\n",
    "valid_dataset = CustomDataset(x_valid, y_valid)  # + ground_truth_scaler.z_transform(baseline[split_seperator:]))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS_TRAIN,\n",
    "                          persistent_workers=PERSISTENT_WORKERS, drop_last=False, prefetch_factor=PREFETCH_FACTOR,\n",
    "                          pin_memory=DEVICE == 'cuda')\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS_VALID,\n",
    "                          persistent_workers=PERSISTENT_WORKERS, drop_last=False, prefetch_factor=PREFETCH_FACTOR,\n",
    "                          pin_memory=DEVICE == 'cuda')"
   ],
   "id": "9e7466651ee27da1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model constants\n",
    "N_ENCODER_LAYERS = 2 # 1\n",
    "N_DECODER_LAYERS = 1\n",
    "D_MODEL = 112 # 112 # 128 # 96\n",
    "N_HEADS = 4\n",
    "D_FEED_FORWARD = 4 * D_MODEL\n",
    "DROPOUT_RATE = 0.1\n",
    "MODEL_NAME = \"m1200\""
   ],
   "id": "5c8e13ff630ea444",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model setup\n",
    "model = Transformer(\n",
    "    num_encoder_layers=N_ENCODER_LAYERS,\n",
    "    num_decoder_layers=N_DECODER_LAYERS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=N_HEADS,\n",
    "    d_ff=D_FEED_FORWARD,\n",
    "    input_features=x_train.shape[1],\n",
    "    output_sequence_length=y_train.shape[1],\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ").to(DEVICE)\n",
    "\n",
    "# ) # mode='max-autotune', dynamic=True , fullgraph=True)\n",
    "model.compile(mode='max-autotune', fullgraph=True)"
   ],
   "id": "2fb7e26c460a795e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# training constants\n",
    "EPOCHS = 5000\n",
    "SCHEDULER_CYCLES = 0.7 # 0.95 # 0.7\n",
    "EARLY_STOPPING_PATIENCE = EPOCHS\n",
    "LEARNING_RATE = 5e-5 # 5e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_PERCENTAGE = 0.01\n",
    "SKIP_VALIDATION_STEPS = 19\n",
    "CLIP_GRAD_NORM = 1000  # 1.0"
   ],
   "id": "1cf7993736224567",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# setup training stuff\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "total_steps = int((len(train_loader) * EPOCHS) / SCHEDULER_CYCLES)\n",
    "warmup_steps = int(total_steps * WARMUP_PERCENTAGE)\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "WITH_ODRMSE = True\n",
    "\n",
    "criterion_mse = nn.MSELoss().to(DEVICE)\n",
    "criterion_odrmse = ODRMSELoss(\n",
    "    total_duration=432 * 10 * 60, times= torch.arange(0, 432, device=DEVICE) * 10 * 60,     # 432 time steps which are scaled to seconds\n",
    "    min_weight_epsilon=1e-5, numerical_stability_delta=5e-7\n",
    ").to(DEVICE)\n",
    "scaler = GradScaler(enabled=(DEVICE == 'cuda'))"
   ],
   "id": "b10577475ca4e7d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# wandb constants - for using wandb use `uv run wandb login`\n",
    "WANDB_PROJECT = \"XXX\"\n",
    "WANDB_ENTITY = \"XXX\"\n",
    "WANDB_RUN_NAME_PREFIX = f\"full-transformer-{MODEL_NAME}\""
   ],
   "id": "9a50f99e39e63a95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# wandb setup\n",
    "config = {\n",
    "    \"model\": \"TimeSeriesTransformer\", \"epochs\": EPOCHS, \"early_stopping_patience\": EARLY_STOPPING_PATIENCE,\n",
    "    \"train_batch_size\": TRAIN_BATCH_SIZE, \"valid_batch_size\": VALID_BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE, \"weight_decay\": WEIGHT_DECAY, \"clip_grad_norm\": CLIP_GRAD_NORM,\n",
    "    \"input_noise_std\": INPUT_NOISE_STD, \"output_noise_std\": OUTPUT_NOISE_STD,\n",
    "    \"mse_weight\": 1.0, \"odrmse_weight\": 1.0 if WITH_ODRMSE else 0.0,\n",
    "    \"d_model\": D_MODEL, \"n_heads\": N_HEADS, \"num_encoder_layers\": N_ENCODER_LAYERS,\n",
    "    \"num_decoder_layers\": N_DECODER_LAYERS,\n",
    "    \"d_ff\": D_FEED_FORWARD, \"dropout\": DROPOUT_RATE, \"total_duration_sec\": 432 * 10 * 60,\n",
    "    \"min_weight_eps\": 1e-5, \"num_stab_delta\": 5e-7,\n",
    "    \"epochal_scaling\": False, \"skip_validation_steps\": SKIP_VALIDATION_STEPS,\n",
    "}\n",
    "run = wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, name=f\"{WANDB_RUN_NAME_PREFIX}\", config=config)\n",
    "\n",
    "summary(model)"
   ],
   "id": "243860ec9d9336b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train helper functions\n",
    "def _save_model(new_best: bool = True):\n",
    "    if new_best:\n",
    "        save_path = weight_save_path / f\"{MODEL_NAME}_best_epoch_{epoch + 1}_valloss_{current_val_loss:.4f}.pt\"\n",
    "        artifact = wandb.Artifact(f'{MODEL_NAME}-best', type='model')\n",
    "        print(f\"  Saving new best model to {save_path}...\")\n",
    "    else:\n",
    "        save_path = weight_save_path / f\"{MODEL_NAME}_epoch_{epoch + 1}_valloss_{current_val_loss:.4f}.pt\"\n",
    "        artifact = wandb.Artifact(f'{MODEL_NAME}', type='model')\n",
    "        print(f\"  Saving model to {save_path}...\")\n",
    "    model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model\n",
    "    torch.save(model_to_save.state_dict(), save_path)\n",
    "\n",
    "    # Optional: Save to W&B Artifacts\n",
    "    artifact.add_file(str(save_path))\n",
    "    wandb.log_artifact(artifact)"
   ],
   "id": "2e30bca670fd6499",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train loop\n",
    "print(f\"\\n--- Starting Training: {MODEL_NAME} ---\")\n",
    "wandb.watch(model, log_freq=100)  # Watch model gradients (optional)\n",
    "best_val_loss = float('inf')\n",
    "best_epoch = -1\n",
    "global_step = 0\n",
    "skipped_validation_steps = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "\n",
    "    # --- Training Phase ---\n",
    "    train_loss_mse_accum = 0.0\n",
    "    train_loss_odrmse_accum = 0.0\n",
    "    train_loss_total_accum = 0.0\n",
    "    processed_samples_train = 0\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} [Train]\", mininterval=0.5)\n",
    "    for batch_data in train_pbar:\n",
    "        # Expects: features, targets\n",
    "        batch_features = batch_data[0].to(DEVICE)\n",
    "        batch_target = batch_data[1].to(DEVICE)\n",
    "\n",
    "        if WITH_ODRMSE:\n",
    "            batch_baseline = None if RESIDUAL_TRAINING else batch_target[:, 0, 1].reshape(-1, 1)\n",
    "            batch_target = batch_target[:, :, 0]\n",
    "\n",
    "        current_batch_size = batch_data[0].size(0)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # --- Forward Pass ---\n",
    "        with autocast(enabled=(DEVICE == 'cuda'), device_type=DEVICE, dtype=torch.float16):\n",
    "            predictions = model(batch_features, batch_target)\n",
    "\n",
    "            # --- Loss Calculation ---\n",
    "            loss_mse = criterion_mse(predictions, batch_target)\n",
    "            if WITH_ODRMSE:\n",
    "                loss_odrmse = criterion_odrmse(predictions, batch_target, batch_baseline)\n",
    "                total_loss = loss_mse + loss_odrmse\n",
    "            else:\n",
    "                total_loss = loss_mse\n",
    "\n",
    "        # --- Backward Pass & Optimization ---\n",
    "        scaler.scale(total_loss).backward()\n",
    "        if CLIP_GRAD_NORM > 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=CLIP_GRAD_NORM)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # --- Logging ---\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        wandb.log({\n",
    "            f\"train/batch_loss_total\": total_loss.item(),\n",
    "            # f\"train/batch_loss_cross-entropy\": loss_mse.item(),\n",
    "            f\"train/batch_loss_mse\": loss_mse.item(),\n",
    "            f\"train/batch_loss_odrmse\": loss_odrmse.item() if WITH_ODRMSE else 0.0,\n",
    "            f\"train/learning_rate\": lr,\n",
    "        }, step=global_step)\n",
    "\n",
    "        train_loss_mse_accum += loss_mse.item() * current_batch_size\n",
    "        train_loss_odrmse_accum += loss_odrmse.item() * current_batch_size if WITH_ODRMSE else 0.0\n",
    "        train_loss_total_accum += total_loss.item() * current_batch_size\n",
    "        processed_samples_train += current_batch_size\n",
    "        global_step += 1\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        train_pbar.set_postfix(loss=f\"{total_loss.item():.4f}\", lr=f\"{lr:.2e}\", refresh=False)\n",
    "\n",
    "    if skipped_validation_steps < SKIP_VALIDATION_STEPS:\n",
    "        skipped_validation_steps += 1\n",
    "        continue\n",
    "\n",
    "    avg_train_loss_mse = train_loss_mse_accum / processed_samples_train\n",
    "    avg_train_loss_odrmse = train_loss_odrmse_accum / processed_samples_train\n",
    "    avg_train_loss_total = train_loss_total_accum / processed_samples_train\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss_mse_accum = 0.0\n",
    "    val_loss_odrmse_accum = 0.0\n",
    "    val_loss_total_accum = 0.0\n",
    "    processed_samples_val = 0\n",
    "\n",
    "    skipped_validation_steps -= SKIP_VALIDATION_STEPS  # this allows for fractions\n",
    "\n",
    "    # Wrap val_loader with tqdm\n",
    "    val_pbar = tqdm(valid_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} [Validate]\")\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_pbar:\n",
    "            # --- Data Preparation (ASSUMES MODIFIED DATALOADER) ---\n",
    "            batch_features = batch_data[0].to(DEVICE)\n",
    "            batch_target = batch_data[1].to(DEVICE)\n",
    "\n",
    "            if WITH_ODRMSE:\n",
    "                batch_baseline = None if RESIDUAL_TRAINING else batch_target[:, 0, 1].reshape(-1, 1)\n",
    "                batch_target = batch_target[:, :, 0]\n",
    "\n",
    "            if torch.isnan(batch_features).any():\n",
    "                print(\"Nan detected in input\")\n",
    "\n",
    "            if torch.isnan(batch_target).any():\n",
    "                print(\"Nan detected in target\")\n",
    "\n",
    "            current_batch_size = batch_target.size(0)\n",
    "\n",
    "            # --- Forward Pass (Inference Mode) ---\n",
    "            with autocast(enabled=(DEVICE == 'cuda'), device_type=DEVICE, dtype=torch.float16):\n",
    "                predictions_eval = model.predict(batch_features)\n",
    "\n",
    "                # --- Loss Calculation (for monitoring) ---\n",
    "                loss_mse = criterion_mse(predictions_eval, batch_target)\n",
    "                if WITH_ODRMSE:\n",
    "                    loss_odrmse = criterion_odrmse(predictions_eval, batch_target, batch_baseline)\n",
    "                    total_loss = loss_mse + loss_odrmse  # Combined for comparison\n",
    "                else:\n",
    "                    total_loss = loss_mse\n",
    "\n",
    "            val_loss_mse_accum += loss_mse.item() * current_batch_size\n",
    "            val_loss_odrmse_accum += loss_odrmse.item() * current_batch_size if WITH_ODRMSE else 0.0\n",
    "            val_loss_total_accum += total_loss.item() * current_batch_size\n",
    "            processed_samples_val += current_batch_size\n",
    "            val_pbar.set_postfix(loss=f\"{total_loss.item():.4f}\")\n",
    "\n",
    "    avg_val_loss_mse = val_loss_mse_accum / processed_samples_val\n",
    "    avg_val_loss_odrmse = val_loss_odrmse_accum / processed_samples_val\n",
    "    avg_val_loss_total = val_loss_total_accum / processed_samples_val\n",
    "\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # --- W&B Epoch Logging ---\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"train/epoch_loss_total\": avg_train_loss_total,\n",
    "        \"train/epoch_loss_mse\": avg_train_loss_mse,\n",
    "        # \"train/epoch_loss_cross-entropy\": avg_train_loss_mse,\n",
    "        \"train/epoch_loss_odrmse\": avg_train_loss_odrmse,\n",
    "        \"val/epoch_loss_total\": avg_val_loss_total,\n",
    "        \"val/epoch_loss_mse\": avg_val_loss_mse,\n",
    "        # \"val/epoch_loss_cross-entropy\": avg_val_loss_mse,\n",
    "        \"val/epoch_loss_odrmse\": avg_val_loss_odrmse,\n",
    "        \"epoch_duration_sec\": epoch_duration,\n",
    "    }, step=global_step)  # Log against the last global step of the epoch\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} Summary:\")\n",
    "    print(\n",
    "        f\"  Train Loss: Total={avg_train_loss_total:.4f}, MSE={avg_train_loss_mse:.4f}, OD-RMSE={avg_train_loss_odrmse:.4f}\")\n",
    "    print(\n",
    "        f\"  Valid Loss: Total={avg_val_loss_total:.4f}, MSE={avg_val_loss_mse:.4f}, OD-RMSE={avg_val_loss_odrmse:.4f}\")\n",
    "    # print(f\"  Train Loss: Total={avg_train_loss_total:.4f}, Cross-Entropy={avg_train_loss_mse:.4f}, OD-RMSE={avg_train_loss_odrmse:.4f}\")\n",
    "    # print(f\"  Valid Loss: Total={avg_val_loss_total:.4f}, Cross-Entropy={avg_val_loss_mse:.4f}, OD-RMSE={avg_val_loss_odrmse:.4f}\")\n",
    "    print(f\"  Epoch Duration: {epoch_duration:.2f} seconds\")\n",
    "\n",
    "    # --- Save Best Model ---\n",
    "    current_val_loss = avg_val_loss_total  # Use combined validation loss\n",
    "\n",
    "    if current_val_loss < best_val_loss:\n",
    "        best_epoch = epoch\n",
    "        best_val_loss = current_val_loss\n",
    "        _save_model()\n",
    "\n",
    "    if best_epoch < epoch - EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"  Early stopping at epoch {epoch + 1} due to lack of improvement in validation loss.\")\n",
    "        break\n",
    "\n",
    "_save_model(False)\n",
    "print(f\"--- Finished Training: {MODEL_NAME} (Best Val Loss: {best_val_loss:.4f}) ---\")"
   ],
   "id": "cae85fb07ed4f77a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now do some evaluation...",
   "id": "502c7d6846ae9588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib\n",
    "\n",
    "model.load_state_dict(torch.load('some-best-model.pt'))\n",
    "model.eval()\n",
    "\n",
    "..."
   ],
   "id": "e308765c6715b0e7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
